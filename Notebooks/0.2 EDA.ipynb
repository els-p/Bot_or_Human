{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Bot or Human?: Exploring Data from Subreddits\n",
    "\n",
    "\n",
    "### Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Read in data scrapped from subreddits](#Read-in-data-scrapped-from-subreddits)\n",
    "- [Data Processing](#Data-Processing)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data scrapped from subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bot = pd.read_csv('../Data/bot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_human = pd.read_csv('../Data/human.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the data, punctuation is removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts(data):\n",
    "    '''\n",
    "    1. Captures punctuation that specifically escapes string.punctuation\n",
    "    2. Converts to lowercase\n",
    "    3. Removes punctuation\n",
    "    '''\n",
    "    data['post_title'] = data['post_title'].str.lower().str.replace('’',':').str.replace('‘',':').str.replace('“',':')\n",
    "    data['post_title_clean'] = data['post_title'].map(lambda x : ''.join(k for k in x if k not in string.punctuation))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [df_bot,df_human]:\n",
    "    clean_posts(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bot = df_bot['post_title_clean']\n",
    "X_human = df_human['post_title_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run the below code to find the top 20 common words in both Subreddits and add the words that are not specific to any one Subreddit to stop words, to filter out during modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words(corpus, num_words):\n",
    "    '''\n",
    "    1. Converts text data into matrix of token counts\n",
    "    2. Sums and sorts words in data, starting from highest count\n",
    "    3. Returns a number of words (num_words) which has the highest counts\n",
    "    '''\n",
    "    cvec = CountVectorizer(max_features=500, stop_words='english')\n",
    "    df = pd.DataFrame(cvec.fit_transform(corpus).todense(),columns=cvec.get_feature_names())\n",
    "    word_counts = df.sum(axis=0)\n",
    "    return word_counts.sort_values(ascending = False).head(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "like      46\n",
       "just      37\n",
       "im        35\n",
       "dont      31\n",
       "time      28\n",
       "vs        25\n",
       "got       24\n",
       "know      24\n",
       "people    24\n",
       "man       24\n",
       "new       22\n",
       "did       20\n",
       "day       20\n",
       "years     19\n",
       "good      19\n",
       "help      18\n",
       "need      18\n",
       "life      16\n",
       "work      16\n",
       "want      16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words(X_bot, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people      112\n",
       "probably     72\n",
       "like         63\n",
       "just         50\n",
       "time         46\n",
       "world        42\n",
       "life         41\n",
       "dont         37\n",
       "really       31\n",
       "know         31\n",
       "years        29\n",
       "youre        28\n",
       "person       27\n",
       "make         24\n",
       "good         24\n",
       "think        24\n",
       "actually     23\n",
       "humans       23\n",
       "earth        22\n",
       "going        22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words(X_human, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
