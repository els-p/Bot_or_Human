{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Bot or Human?: Reddit Scraper\n",
    "\n",
    "I use the Reddit API to gather data from two subreddit threads: \n",
    "<br>(1) [SubredditsSimulator](https://www.reddit.com/r/SubredditSimulator/) and;\n",
    "<br>(2)  [Showerthoughts](https://www.reddit.com/r/Showerthoughts/)\n",
    "\n",
    "Reddit caps scrapping result at 25 posts per request. To get enough data, I iterate through a number of pages to get the maximum number of posts allowable which is 1000, depending on whether there are that many post in the subreddit threads. The datasets are then exported to csv for analysis.\n",
    "\n",
    "\n",
    "### Contents:\n",
    "- [Import Libraries](#Import-Libraries)\n",
    "- [Get posts from SubredditSimulator](#Get-posts-from-SubredditSimulator)\n",
    "- [Get posts from Showerthoughts](#Get-posts-from-Showerthoughts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get posts from SubredditSimulator\n",
    "\n",
    "Only titles of posts and name of users are saved to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set header to prevent 429 status code\n",
    "header = {'User-agent': 'ep 0.1.1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abfe9ae3a654184bd6c18c555dcedf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=34), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully scrapped 852 unique posts\n"
     ]
    }
   ],
   "source": [
    "#Set empty list to store posts\n",
    "posts = []\n",
    "\n",
    "#Set param as none for first iteration\n",
    "after = None\n",
    "\n",
    "#Iterate through 34 pages of 25 posts to get max number of posts\n",
    "for i in tqdm(range(34)):\n",
    "    if after == None:\n",
    "        param = {}\n",
    "    else:\n",
    "        param = {'after': after}\n",
    "    url = 'https://www.reddit.com/r/SubredditSimulator/.json'\n",
    "    results = requests.get(url, params=param, headers=header)\n",
    "    if results.status_code == 200: #Check if request successful\n",
    "        res_json = results.json()\n",
    "        posts.extend(res_json['data']['children'])\n",
    "        after = res_json['data']['after']\n",
    "    else:\n",
    "        print(results.status_code)\n",
    "        break\n",
    "    #Rest time in seconds\n",
    "    time.sleep(1)\n",
    "\n",
    "#Records only posts from unique users\n",
    "posts = pd.DataFrame(posts)\n",
    "lst = {}\n",
    "lst['post_title'] =[]\n",
    "lst['name'] =[]\n",
    "for i in posts['data']:\n",
    "    if i['name'] not in lst['name']:\n",
    "        lst['post_title'].append(i['title'])\n",
    "        lst['name'].append(i['name'])\n",
    "\n",
    "print('Successfully scrapped {} unique posts'.format(len(lst['post_title'])))\n",
    "scrapped_bot = pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as \"bot.csv\" in output folder\n"
     ]
    }
   ],
   "source": [
    "#Output data\n",
    "scrapped_bot.to_csv('../output/bot.csv', index=False)\n",
    "print('File saved as \"bot.csv\" in output folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get posts from Showerthoughts\n",
    "\n",
    "Only titles of posts and name of users are saved to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cddd20c0fc64513bc6f484ec61bde90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=40), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully scrapped 839 unique posts\n"
     ]
    }
   ],
   "source": [
    "#Set empty list to store posts\n",
    "posts = []\n",
    "\n",
    "#Set param as none for first iteration\n",
    "after = None\n",
    "\n",
    "#Iterate through 40 pages of 25 posts to get max number of posts\n",
    "for i in tqdm(range(40)):\n",
    "    if after == None:\n",
    "        param = {}\n",
    "    else:\n",
    "        param = {'after': after}\n",
    "    url = 'https://www.reddit.com/r/Showerthoughts/.json'\n",
    "    results = requests.get(url, params=param, headers=header)\n",
    "    if results.status_code == 200: #Check if request successful\n",
    "        res_json = results.json()\n",
    "        posts.extend(res_json['data']['children'])\n",
    "        after = res_json['data']['after']\n",
    "    else:\n",
    "        print(results.status_code)\n",
    "        break\n",
    "    #Rest time in seconds\n",
    "    time.sleep(1)\n",
    "\n",
    "#Records only posts from unique users\n",
    "posts = pd.DataFrame(posts)\n",
    "lst = {}\n",
    "lst['post_title'] =[]\n",
    "lst['name'] =[]\n",
    "for i in posts['data']:\n",
    "    if i['name'] not in lst['name']:\n",
    "        lst['post_title'].append(i['title'])\n",
    "        lst['name'].append(i['name'])\n",
    "\n",
    "print('Successfully scrapped {} unique posts'.format(len(lst['post_title'])))\n",
    "scrapped_human = pd.DataFrame(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as \"human.csv\" in output folder\n"
     ]
    }
   ],
   "source": [
    "#Output data\n",
    "scrapped_human.to_csv('../output/human.csv', index=False)\n",
    "print('File saved as \"human.csv\" in output folder')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
